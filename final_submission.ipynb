{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Academic declaration\n",
    "\n",
    "I have read and understood the sections on plagiarism in the College Policy on assessment offences and confirm that the work is my own, with the work of others clearly acknowledged. I give my permission to submit my work to the plagiarism testin database that the College is using and test it using plagiarism detection software, search engines or meta-searching software.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud computing submission\n",
    "\n",
    "Author: Dorian Beganovic\n",
    "\n",
    "Contents of the Jupyter notebook:\n",
    "\n",
    "1) Conditional probabilities using Stripes pattern  \n",
    "2) Conditional probabilities using Pairs pattern  \n",
    "3) Page rank using the \"simplified\" page rank algorithm  \n",
    "4) Page rank using the \"complete\" page rank algorithm  \n",
    "\n",
    "\n",
    "- Explanation of other submitted files\n",
    "\t- *.jpg pictures* in *final_screenshots* folder - the screenshots for each job consisting of\n",
    "\t\t- Screenshot from AWS EMR\n",
    "\t\t\t- Execution of first step in the EMR console\n",
    "\t\t\t- Execution of last step in the EMR console\n",
    "\t\t- Screenshot from my command line\n",
    "\t\t\t- Execution of first step from the command line\n",
    "\t\t\t- Execution of last step from the command line\n",
    "\t\t- Screenshot of S3 output\n",
    "\t\t- Log (syslog) of the last step in EMR\n",
    "\t- *report.txt* - the document explaining how much time each program took to run on EMR and how much time I spent working on this problem\n",
    "\t- *mrjob.conf* - the mrjob configuration file used\n",
    "\t- *execute.sh* - commands used to execute each of the tasks\n",
    "\t- *count_nodes.py* - script to get the minimum node id, maximum node id and the number of unique nodes\n",
    "\t- *test_bigrams.py* - script that reads the \"shortjokes.csv\" file and calculates bigram probabilities. It output matches both the stripes and pairs output for the bigrams starting with \"my\"\n",
    "\t- *test_pagerank_scores.py* - script that operates on the output of page rank jobs by summing up the page rank values. This is to ensure that the complete page rank keeps the sum of page rank at 1 (since all nodes are initialised with 1/num_nodes)\n",
    "\t- *test_pageranks.py* - script that utilises the networkx python package to calculate the page ranks. It operates using a more complex method so the page ranks between this method and my complete page rank don't exactly match but the top 20 nodes by page rank are identical\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Conditional probabilities using Stripes pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob, MRStep\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import operator\n",
    "\n",
    "\n",
    "class MRWordBigramProb(MRJob):\n",
    "    def mapper_init(self):\n",
    "        \"\"\"\n",
    "        Initialise the dictionary for \"in-mapper\" combining\n",
    "        where key will be a word and values are a list\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.following_words = defaultdict(lambda: [])\n",
    "\n",
    "    def mapper(self, joke_id, content):\n",
    "        \"\"\"\n",
    "        For each encountered word in the text, add a entry to the list of following words.\n",
    "\n",
    "        We ignore the last word in a sentence as it has not following words.\n",
    "\n",
    "        :param joke_id: ignored\n",
    "        :param content:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        words = re.sub('\\s+', ' ', re.sub('[^a-z]+', ' ', re.sub('\\'', '', content.lower()))).strip(' ').split(\" \")\n",
    "        for index, word in enumerate(words):\n",
    "            if index < len(words) - 1:\n",
    "                self.following_words[word].append(words[index+1])\n",
    "\n",
    "    def mapper_final(self):\n",
    "        \"\"\"\n",
    "        Create a \"stripe\" out of the list of the succeeding words for each word.\n",
    "\n",
    "        For each word, loops over all the following words (stored in a list) and places them into\n",
    "        a dictionary which counts the numbers of their occurrences.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for word in self.following_words.keys():\n",
    "            bigram_count = defaultdict(lambda: 0)\n",
    "            for following_word in self.following_words[word]:\n",
    "                bigram_count[following_word] += 1\n",
    "            yield word, bigram_count\n",
    "\n",
    "    def reducer_init(self):\n",
    "        \"\"\"\n",
    "        Keeps state over multiple reduce iterations, required for sorting the final output\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.bigram_frequencies = []\n",
    "\n",
    "    def reducer(self, word, stripes):\n",
    "        \"\"\"\n",
    "        Adds up the individual stripes from each reducer\n",
    "        and then calculates the conditional probability\n",
    "\n",
    "        :param word:\n",
    "        :param stripes:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        counts = Counter()\n",
    "        for stripe in stripes:\n",
    "            counts.update(stripe)\n",
    "        count_total = sum(counts.values())\n",
    "        for following_word in counts:\n",
    "            self.bigram_frequencies.append((word, following_word, (counts[following_word] / count_total)))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        \"\"\"\n",
    "        Sorts the triples of (word, following word, frequency of the bigram)\n",
    "        based word and then frequency of the bigram (word, following word).\n",
    "        Outputs sorted values.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sorted_bigram_frequencies = sorted(self.bigram_frequencies, key=operator.itemgetter(0, 2))\n",
    "        for bigram in sorted_bigram_frequencies:\n",
    "            yield bigram[0] + \"-\" + bigram[1], bigram[2]\n",
    "\n",
    "    def steps(self):\n",
    "        steps = [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final\n",
    "            )\n",
    "        ]\n",
    "        return steps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordBigramProb.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most likely words after \"my\" using the Stripes pattern:\n",
    "\n",
    "| Rank | Word | Probability   |  \n",
    "|------|------|------|  \n",
    "|1|\"my-wife\" |\t0.05205927439959121  \n",
    "|2|\"my-girlfriend\" |\t0.03613694430250383  \n",
    "|3|\"my-friend\" |\t0.026448645886561064  \n",
    "|4|\"my-dad\" |\t0.01598364844149208  \n",
    "|5|\"my-women\" |\t0.01483903934593766  \n",
    "|6|\"my-life\" |\t0.012692897291773123  \n",
    "|7|\"my-mom\" |\t0.011016862544711292  \n",
    "|8|\"my-favorite\" |\t0.010975983648441491  \n",
    "|9|\"my-coffee\" |\t0.01093510475217169  \n",
    "|10|\"my-son\" |\t0.010689831374552887  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Conditional probabilities using Pairs pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob, MRStep\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "\n",
    "class MRWordBigramProb(MRJob):\n",
    "    #PARTITIONER = 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def mapper_init(self):\n",
    "        \"\"\"\n",
    "        Initialises the dictionary used for \"in-mapper\" combining\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.pair_counts = defaultdict(lambda: 0)\n",
    "\n",
    "    def mapper(self, joke_id, content):\n",
    "        \"\"\"\n",
    "        For each encountered word in joke, it increments the count for the bigram\n",
    "        with it's following word.\n",
    "        it also adds the value to \"*\"\n",
    "\n",
    "        Note that we ignore the last word in a sentence as it has no following words.\n",
    "\n",
    "        :param joke_id: ignored\n",
    "        :param content: the raw content of the document\n",
    "        :return: tuple of ((word, following_word),1) or ((word,*))\n",
    "        \"\"\"\n",
    "        words = re.sub('\\s+', ' ', re.sub('[^a-z]+', ' ', re.sub('\\'', '', content.lower()))).strip(' ').split(\" \")\n",
    "        for index, word in enumerate(words):\n",
    "            if index < len(words) - 1:\n",
    "                self.pair_counts[(word, \"*\")] += 1  # will be used to calculated the total number of occurrences of the word\n",
    "                self.pair_counts[(word, words[index+1])] += 1\n",
    "\n",
    "    def mapper_final(self):\n",
    "        \"\"\"\n",
    "        Yields the pairs after they have combined.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for pair in self.pair_counts.keys():\n",
    "            yield pair, self.pair_counts[pair]\n",
    "\n",
    "    def reducer_init(self):\n",
    "        \"\"\"\n",
    "        Keeps state over multiple reduce iterations, very important in order to\n",
    "        calculate the \"total_word_count\" for each 1st word of the bigram\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.total_word_count = {}\n",
    "        self.bigram_frequencies = []\n",
    "\n",
    "    def reducer(self, pair, counts):\n",
    "        \"\"\"\n",
    "        Expects that the input key (word, following_word) is partitioned by 1st part of the key (word)\n",
    "        and sorted by the 2nd key (following word).\n",
    "        It expects that \"*\" will be the first following word for every word.\n",
    "\n",
    "        This is exactly specified in the config.\n",
    "\n",
    "        Outputs a triple containing (word, following word, conditional probability).\n",
    "\n",
    "        :param pair: a bigram (word, following_word)\n",
    "        :param counts: sum of counts for every bigram on every node\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        word = pair[0]\n",
    "        following_word = pair[1]\n",
    "        if following_word == \"*\":\n",
    "            self.total_word_count[word] = sum(counts)\n",
    "        else:\n",
    "            self.bigram_frequencies.append((word, following_word, (sum(counts) / self.total_word_count[word])))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        \"\"\"\n",
    "        Sorts the triples of (word, following word, frequency of the bigram)\n",
    "        based word and then frequency of the bigram (word, following word).\n",
    "        Outputs sorted values.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sorted_bigram_frequencies = sorted(self.bigram_frequencies, key=operator.itemgetter(0, 2))\n",
    "        for bigram in sorted_bigram_frequencies:\n",
    "            yield bigram[0] + \"-\" + bigram[1], bigram[2]\n",
    "\n",
    "    def steps(self):\n",
    "        # While these configs make sense, they are not necessary when SORT_VALEUS=True is used\n",
    "        #   https://mrjob.readthedocs.io/en/latest/job.html#mrjob.job.MRJob.SORT_VALUES\n",
    "        # jobconf = {\n",
    "        #     'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "        #     'mapred.text.key.comparator.options': '-k2r',\n",
    "        #     'mapred.text.key.partitioner.options': '-k1',\n",
    "        #     'stream.num.map.output.key.fields': 2,\n",
    "        #     'stream.map.output.field.separator' : \"\\t\",\n",
    "        #     'mapreduce.map.output.key.field.separator': '\\t',\n",
    "        # }\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_final=self.reducer_final,\n",
    "                   # jobconf=jobconf\n",
    "            )\n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordBigramProb().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most likely words after \"my\" using the Pairs pattern:\n",
    "\n",
    "| Rank | Word | Probability   |  \n",
    "|------|------|------|  \n",
    "|1|\"my-wife\" |\t0.05205927439959121  \n",
    "|2|\"my-girlfriend\" |\t0.03613694430250383  \n",
    "|3|\"my-friend\" |\t0.026448645886561064  \n",
    "|4|\"my-dad\" |\t0.01598364844149208  \n",
    "|5|\"my-women\" |\t0.01483903934593766  \n",
    "|6|\"my-life\" |\t0.012692897291773123  \n",
    "|7|\"my-mom\" |\t0.011016862544711292  \n",
    "|8|\"my-favorite\" |\t0.010975983648441491  \n",
    "|9|\"my-coffee\" |\t0.01093510475217169  \n",
    "|10|\"my-son\" |\t0.010689831374552887  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Page rank using the \"simplified\" page rank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob, MRStep\n",
    "from mrjob.protocol import TextProtocol\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "\n",
    "class MRPageRank(MRJob):\n",
    "    INPUT_PROTOCOL = TextProtocol\n",
    "\n",
    "    def configure_args(self):\n",
    "        \"\"\"\n",
    "        We configure:\n",
    "        - n_iterations - number of iterations of page rank\n",
    "        - n_nodes - number of nodes in the graph\n",
    "        - top_n - how many of the top results we want\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(MRPageRank, self).configure_args()\n",
    "        self.add_passthru_arg(\n",
    "            '--n_iterations', type=int, default=10, help='Number of iterations of page rank to run')\n",
    "        self.add_passthru_arg(\n",
    "            '--n_nodes', type=int, default=75879, help='Number of nodes in the graph')\n",
    "        self.add_passthru_arg(\n",
    "            '--top_n', type=int, default=20, help='Number of top pages to output')\n",
    "\n",
    "    def convert_edge_list_to_adjacency_list_reducer(self, node_id, nodes):\n",
    "        \"\"\"\n",
    "        Converts the edge list (node_a, node_b) into a single node\n",
    "        with adjacency list ({out_links = [a,b,c], page_rank = pr})\n",
    "\n",
    "        Furthermore, we output all outgoing nodes with a placeholder \"*\"\n",
    "        to initialise the dangling nodes in the following reducer\n",
    "\n",
    "        :param node_id: the node id which is the key in reduce step\n",
    "        :param nodes: list of node id's\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node = dict()\n",
    "        node['out_links'] = list(nodes)\n",
    "        node['page_rank'] = 1 / self.options.n_nodes  # default values\n",
    "        yield node_id, node\n",
    "\n",
    "        for out_node_id in node['out_links']:\n",
    "            yield out_node_id, \"*\"\n",
    "\n",
    "    def convert_edge_list_to_adjacency_list_reducer_dangling(self, node_id, nodes):\n",
    "        \"\"\"\n",
    "        The previous reducer outputs a node id and either \"*\" or the nodes structure as value.\n",
    "\n",
    "        This reducer doesn't change the nodes that have a node structure as value.\n",
    "        For nodes that only have \"*\" as value, it constructs a dangling node.\n",
    "\n",
    "        :param node_id: the node id which is the key in reduce step\n",
    "        :param nodes: list \"*\" or node structure\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        real_node = None\n",
    "        for node in nodes:\n",
    "            if node == \"*\":\n",
    "                pass\n",
    "            elif isinstance(node, dict):\n",
    "                real_node = node\n",
    "            else:\n",
    "                raise ValueError(\"Shouldn't happen\")\n",
    "\n",
    "        if real_node is None:\n",
    "            real_node = dict()\n",
    "            real_node['out_links'] = []\n",
    "            real_node['page_rank'] = 1 / self.options.n_nodes\n",
    "\n",
    "        yield node_id, real_node\n",
    "\n",
    "    def map_page_rank_contribution_init(self):\n",
    "        \"\"\"\n",
    "        To utilise the \"in-mapper\" combiner pattern, initialises a dictionary\n",
    "        for holding nodes and their associated sums of incoming page ranks:\n",
    "            - {node_id: [sum of incoming page rank contributions] , ...}\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.incoming_page_ranks = defaultdict(lambda: 0)\n",
    "\n",
    "    def map_page_rank_contribution(self, node_id, node):\n",
    "        \"\"\"\n",
    "        For every node id and it's associated node structure:\n",
    "        1. divides the page rank of the node by the total number of outgoing links\n",
    "        2. stores the page rank contribution to every outgoing link (node)\n",
    "            which later gets emited using the \"in-mapper\" combiner patter\n",
    "        3. emits the inputs node_id and node_structure\n",
    "\n",
    "        :param node_id: node id\n",
    "        :param node: node structure in JSON format\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(node['out_links']) > 0:\n",
    "            page_rank_contribution = node['page_rank'] / len(node['out_links'])\n",
    "            for out_link_node_id in node['out_links']:\n",
    "                self.incoming_page_ranks[out_link_node_id] += page_rank_contribution\n",
    "        yield node_id, node\n",
    "\n",
    "    def map_page_rank_contribution_final(self):\n",
    "        \"\"\"\n",
    "        For every stored node_id, returns combined incoming page rank contributions\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for node_id in self.incoming_page_ranks.keys():\n",
    "            yield node_id, self.incoming_page_ranks[node_id]\n",
    "\n",
    "    def reduce_incoming_page_rank_contributions(self, node_id, page_rank_contributions):\n",
    "        \"\"\"\n",
    "        Sums up all incoming page rank contributions for a node without taking into account the\n",
    "        loss of page rank mass due to dangling nodes and \"teleportation\".\n",
    "\n",
    "        Yields a node with the updated page rank value.\n",
    "\n",
    "        :param node_id: node\n",
    "        :param page_rank_contributions: list of contribution values and the node structure\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        node = None\n",
    "        page_rank_sum = 0\n",
    "        for page_rank_contribution in page_rank_contributions:\n",
    "            if isinstance(page_rank_contribution, float):\n",
    "                page_rank_sum += page_rank_contribution\n",
    "            else:\n",
    "                # this is the case where we send the actual node JSON structure\n",
    "                node = page_rank_contribution\n",
    "\n",
    "        node['page_rank'] = page_rank_sum\n",
    "        yield node_id, node\n",
    "\n",
    "    def topN_mapper_init(self):\n",
    "        \"\"\"\n",
    "        Stores all the page ranks received in the mapper\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.values = []\n",
    "\n",
    "    def topN_mapper(self, node_id, node):\n",
    "        \"\"\"\n",
    "        Reverts the order so that page ranks are keys.\n",
    "        This is used together with a custom comparator in the shuffle stage before the next reducer\n",
    "\n",
    "        :param node_id:\n",
    "        :param node:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.values.append((node['page_rank'], node_id))\n",
    "\n",
    "    def topN_mapper_final(self):\n",
    "        \"\"\"\n",
    "        Returns N largest values based on page rank as (page rank, node id)\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for pair in heapq.nlargest(n=self.options.top_n, iterable=self.values, key=lambda x: x[0]):\n",
    "            yield pair\n",
    "\n",
    "    def topN_reducer_init(self):\n",
    "        \"\"\"\n",
    "        Keeps track of how many top page ranks were iterated over\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.values = []\n",
    "\n",
    "    def topN_reducer(self, page_rank, node_ids):\n",
    "        \"\"\"\n",
    "        Outputs the top N nodes along with their page ranks\n",
    "\n",
    "        :param page_rank:\n",
    "        :param node_ids:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for node_id in node_ids:\n",
    "            self.values.append((page_rank, node_id))\n",
    "\n",
    "    def topN_reducer_final(self):\n",
    "        \"\"\"\n",
    "        Returns N largest values based on page rank as (page rank, node id)\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for pair in heapq.nlargest(n=self.options.top_n, iterable=self.values, key=lambda x: x[0]):\n",
    "            yield pair\n",
    "\n",
    "    def steps(self):\n",
    "        steps = [MRStep(reducer=self.convert_edge_list_to_adjacency_list_reducer)] + \\\n",
    "                [MRStep(reducer=self.convert_edge_list_to_adjacency_list_reducer_dangling)] + \\\n",
    "                [MRStep(\n",
    "                    mapper_init=self.map_page_rank_contribution_init,\n",
    "                    mapper=self.map_page_rank_contribution,\n",
    "                    mapper_final=self.map_page_rank_contribution_final,\n",
    "                    reducer=self.reduce_incoming_page_rank_contributions)\n",
    "                ] * self.options.n_iterations + \\\n",
    "                [MRStep(mapper_init=self.topN_mapper_init,\n",
    "                        mapper=self.topN_mapper,\n",
    "                        mapper_final=self.topN_mapper_final,\n",
    "                        reducer_init=self.topN_reducer_init,\n",
    "                        reducer=self.topN_reducer,\n",
    "                        reducer_final=self.topN_reducer_final,\n",
    "                        jobconf={\n",
    "                            'mapred.reduce.tasks': 1,\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1nr'\n",
    "                        })\n",
    "                 ]\n",
    "\n",
    "        return steps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRPageRank.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 page ranks using the \"simplified\" page rank algorithm and 10 iterations:\n",
    "\n",
    "| Rank | Node | Page rank   |  \n",
    "|------|------|------|  \n",
    "|1| \"18\" |  0.003130077602885494  \n",
    "|2| \"737\" |  0.002863553011238143  \n",
    "|3| \"40\" |  0.001871953130794608  \n",
    "|4| \"118\" |  0.0017995720480840556  \n",
    "|5| \"401\"  | 0.0015652778739008069  \n",
    "|6| \"725\" | 0.001514956814924569  \n",
    "|7| \"1719\" |  0.0015039516052128294  \n",
    "|8| \"136\" |  0.001482907684427656  \n",
    "|9| \"849\" |  0.0014773067290279006  \n",
    "|10| \"550\"  | 0.0014122234888086457  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Page rank using the \"complete\" page rank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob, MRStep\n",
    "from mrjob.protocol import TextProtocol\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "\n",
    "class MRPageRank(MRJob):\n",
    "    INPUT_PROTOCOL = TextProtocol\n",
    "\n",
    "    def configure_args(self):\n",
    "        \"\"\"\n",
    "        We configure:\n",
    "        - n_iterations - number of iterations of page rank\n",
    "        - damping_factor - the damping factor from the page rank equation\n",
    "        - n_nodes - number of nodes in the graph\n",
    "        - min_node_id - the minimal node id\n",
    "        - max_node_id - the maximal node id\n",
    "        - top_n - how many of the top results we want\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        super(MRPageRank, self).configure_args()\n",
    "        self.add_passthru_arg(\n",
    "            '--n_iterations', type=int, default=10, help='Number of iterations of page rank to run')\n",
    "        self.add_passthru_arg(\n",
    "            '--damping_factor', type=float, default=0.85, help='The damping factor using in the calculations')\n",
    "        self.add_passthru_arg(\n",
    "            '--n_nodes', type=int, default=75879, help='Number of nodes in the graph (computed using count_nodes.py)')\n",
    "        self.add_passthru_arg(\n",
    "            '--min_node_id', type=int, default=0, help='Minimal node id (computed using count_nodes.py)')\n",
    "        self.add_passthru_arg(\n",
    "            '--max_node_id', type=int, default=75887, help='Maximal node id (computed using count_nodes.py)')\n",
    "        self.add_passthru_arg(\n",
    "            '--top_n', type=int, default=10, help='Number of top pages to output')\n",
    "\n",
    "    def convert_edge_list_to_adjacency_list_reducer(self, node_id, nodes):\n",
    "        \"\"\"\n",
    "        Converts the edge list (node_a, node_b) into a single node\n",
    "        with adjacency list ({out_links = [a,b,c], page_rank = pr})\n",
    "\n",
    "        Furthermore, we output all outgoing nodes with a placeholder \"*\"\n",
    "        to initialise the dangling nodes in the following reducer\n",
    "\n",
    "        :param node_id: the node id which is the key in reduce step\n",
    "        :param nodes: list of node id's\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node = dict()\n",
    "        node['out_links'] = list(nodes)\n",
    "        node['page_rank'] = 1/self.options.n_nodes\n",
    "        yield node_id, node\n",
    "        for out_node_id in node['out_links']:\n",
    "            yield out_node_id, \"*\"\n",
    "\n",
    "    def convert_edge_list_to_adjacency_list_reducer_dangling(self, node_id, nodes):\n",
    "        \"\"\"\n",
    "        The previous reducer outputs a node id and either \"*\" or the nodes structure as value.\n",
    "\n",
    "        This reducer doesn't change the nodes that have a node structure as value.\n",
    "        For nodes that only have \"*\" as value, it constructs a dangling node.\n",
    "\n",
    "        :param node_id: the node id which is the key in reduce step\n",
    "        :param nodes: list \"*\" or node structure\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        real_node = None\n",
    "        for node in nodes:\n",
    "            if node == \"*\":\n",
    "                pass\n",
    "            elif isinstance(node, dict):\n",
    "                real_node = node\n",
    "            else:\n",
    "                raise ValueError(\"Shouldn't happen\")\n",
    "\n",
    "        if real_node is None:\n",
    "            real_node = dict()\n",
    "            real_node['out_links'] = []\n",
    "            real_node['page_rank'] = 1/self.options.n_nodes\n",
    "\n",
    "        yield node_id, real_node\n",
    "\n",
    "    def map_page_rank_contribution_init(self):\n",
    "        \"\"\"\n",
    "        To utilise the \"in-mapper\" combiner pattern, initialises a dictionary\n",
    "        for holding nodes and their associated sums of incoming page ranks:\n",
    "            - {node_id: [sum of incoming page rank contributions] , ...}\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.incoming_page_ranks = defaultdict(lambda: 0)\n",
    "        self.dangling_pr_mass = 0\n",
    "\n",
    "    def map_page_rank_contribution(self, node_id, node):\n",
    "        \"\"\"\n",
    "        For every node id and it's associated node structure:\n",
    "        1.\n",
    "        A) if it's not  dangling node:\n",
    "            1. divides the page rank of the node by the total number of outgoing links\n",
    "            2. stores the page rank contribution to every outgoing link (node)\n",
    "                which later gets emited using the \"in-mapper\" combiner patter\n",
    "        B) if it's a dangling node\n",
    "            1. store it's mass together with the mass of other dangling nodes\n",
    "        2. emits the inputs node_id and node_structure\n",
    "\n",
    "        :param node_id: node id\n",
    "        :param node: node structure in JSON format\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(node['out_links']) > 0:\n",
    "            page_rank_contribution = node['page_rank'] / len(node['out_links'])\n",
    "            for out_link_node_id in node['out_links']:\n",
    "                self.incoming_page_ranks[out_link_node_id] += page_rank_contribution\n",
    "        elif len(node['out_links']) == 0:\n",
    "            self.dangling_pr_mass += node['page_rank']\n",
    "        yield node_id, node\n",
    "\n",
    "    def map_page_rank_contribution_final(self):\n",
    "        \"\"\"\n",
    "        For every stored node_id, returns combined incoming page rank contributions.\n",
    "\n",
    "        Returns the page rank mass of all dangling nodes in this reducer using a special \"d\" key\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for node_id in self.incoming_page_ranks.keys():\n",
    "            yield node_id, self.incoming_page_ranks[node_id]\n",
    "        yield 'd', self.dangling_pr_mass\n",
    "\n",
    "    def reduce_incoming_page_rank_contributions(self, node_id, page_rank_contributions):\n",
    "        \"\"\"\n",
    "        1)\n",
    "            A) If the incoming node is the sum of dangling masses,\n",
    "                then it loops through the range on all node id's and emits a page rank contribution to each node\n",
    "            B) If the node is a normal node, it just sums up the contributions, updates the nodes pages rank\n",
    "                and outputs the node structure\n",
    "\n",
    "        :param node_id: node id\n",
    "        :param page_rank_contributions: list of contributions -> either (\"d\",<mass>), (<node id>, <mass>), (<node id>, <node structure>)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if node_id == 'd':\n",
    "            dangling_mass = sum(page_rank_contributions)\n",
    "            for node_id in range(self.options.min_node_id, self.options.max_node_id):\n",
    "                yield str(node_id), dangling_mass\n",
    "        else:\n",
    "            node = None\n",
    "            page_rank_sum = 0\n",
    "            for page_rank_contribution in page_rank_contributions:\n",
    "                if isinstance(page_rank_contribution, float):\n",
    "                    page_rank_sum += page_rank_contribution\n",
    "                else:\n",
    "                    # this is the case where we send the actual node JSON structure\n",
    "                    node = page_rank_contribution\n",
    "            node['page_rank'] = page_rank_sum\n",
    "            yield node_id, node\n",
    "\n",
    "    def complete_page_rank_reducer(self, node_id, dangling_nodes_contributions):\n",
    "        \"\"\"\n",
    "        1. Sums up all the incoming contributions from the dangling nodes (emitted in previous reducer)\n",
    "        2. Properly updates the pagerank to take into account the damping factor and dangling mass\n",
    "\n",
    "        :param node_id:\n",
    "        :param dangling_nodes_contributions: either (<node id>, <dangling mass contribution>), (<node id>, <node structure>)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node = None\n",
    "        dangling_contribution = 0\n",
    "        for i, dangling_nodes_contribution in enumerate(dangling_nodes_contributions):\n",
    "            if isinstance(dangling_nodes_contribution, float):\n",
    "                dangling_contribution += dangling_nodes_contribution\n",
    "            else:\n",
    "                # this is the case where we send the actual node JSON structure\n",
    "                node = dangling_nodes_contribution\n",
    "\n",
    "            # dangling nodes do not have the graph node structure\n",
    "\n",
    "        if node is not None:\n",
    "            old_page_rank = node['page_rank']\n",
    "            updated_page_rank = ((1 - self.options.damping_factor) / self.options.n_nodes) \\\n",
    "                + (self.options.damping_factor * (dangling_contribution / self.options.n_nodes + old_page_rank))\n",
    "            node['page_rank'] = updated_page_rank\n",
    "\n",
    "            yield node_id, node\n",
    "        else:\n",
    "            # these are the 9 missing node id's in the range min_node_id to max_node_id\n",
    "            pass\n",
    "\n",
    "    def topN_mapper_init(self):\n",
    "        \"\"\"\n",
    "        Stores all the page ranks received in the mapper\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.values = []\n",
    "\n",
    "    def topN_mapper(self, node_id, node):\n",
    "        \"\"\"\n",
    "        Reverts the order so that page ranks are keys.\n",
    "        This is used together with a custom comparator in the shuffle stage before the next reducer\n",
    "\n",
    "        :param node_id:\n",
    "        :param node:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.values.append((node['page_rank'], node_id))\n",
    "\n",
    "    def topN_mapper_final(self):\n",
    "        \"\"\"\n",
    "        Returns N largest values based on page rank as (page rank, node id)\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for pair in heapq.nlargest(n=self.options.top_n, iterable=self.values, key=lambda x: x[0]):\n",
    "            yield pair\n",
    "\n",
    "    def topN_reducer_init(self):\n",
    "        \"\"\"\n",
    "        Keeps track of how many top page ranks were iterated over\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.values = []\n",
    "\n",
    "    def topN_reducer(self, page_rank, node_ids):\n",
    "        \"\"\"\n",
    "        Outputs the top N nodes along with their page ranks\n",
    "\n",
    "        :param page_rank:\n",
    "        :param node_ids:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for node_id in node_ids:\n",
    "            self.values.append((page_rank, node_id))\n",
    "\n",
    "    def topN_reducer_final(self):\n",
    "        \"\"\"\n",
    "        Returns N largest values based on page rank as (page rank, node id)\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for pair in heapq.nlargest(n=self.options.top_n, iterable=self.values, key=lambda x: x[0]):\n",
    "            yield pair\n",
    "\n",
    "    def steps(self):\n",
    "        steps = [MRStep(reducer=self.convert_edge_list_to_adjacency_list_reducer)] \\\n",
    "                + [ MRStep(reducer=self.convert_edge_list_to_adjacency_list_reducer_dangling)] \\\n",
    "                + [\n",
    "                    MRStep(\n",
    "                        mapper_init=self.map_page_rank_contribution_init,\n",
    "                        mapper=self.map_page_rank_contribution,\n",
    "                        mapper_final=self.map_page_rank_contribution_final,\n",
    "                        reducer=self.reduce_incoming_page_rank_contributions\n",
    "                    ),\n",
    "                    MRStep(\n",
    "                        reducer=self.complete_page_rank_reducer)\n",
    "                ] * self.options.n_iterations \\\n",
    "                + [MRStep(mapper_init=self.topN_mapper_init,\n",
    "                        mapper=self.topN_mapper,\n",
    "                        mapper_final=self.topN_mapper_final,\n",
    "                        reducer_init=self.topN_reducer_init,\n",
    "                        reducer=self.topN_reducer,\n",
    "                        reducer_final=self.topN_reducer_final,\n",
    "                        jobconf={\n",
    "                            'mapred.reduce.tasks': 1,\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1nr'\n",
    "                        })\n",
    "                 ]\n",
    "\n",
    "        return steps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRPageRank.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 page ranks using the \"complete\" page rank algorithm and 50 iterations:\n",
    "\n",
    "\n",
    "| Rank | Node | Page rank   |  \n",
    "|------|------|------|  \n",
    "|1| \"18\" |  0.004535027430895477  \n",
    "|2| \"737\" |  0.003150401858160732  \n",
    "|3| \"118\" |  0.0021219790121769088  \n",
    "|4| \"1719\" |  0.002078180983269449  \n",
    "|5| \"136\"  | 0.0019870699856579417  \n",
    "|6| \"790\" | 0.0019688909929434454  \n",
    "|7| \"143\" |  0.001956844315787098  \n",
    "|8| \"40\" |  0.0018248551665789362  \n",
    "|9| \"1619\" |  0.0015362428791861633  \n",
    "|10| \"725\"  | 0.0014960008685869172  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
